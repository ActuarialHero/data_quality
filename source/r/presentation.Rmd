---
title: "Using Open Source Tools to Evaluate Insurance Data Quality"
author: "Kevin McBeth, FCAS, MAAA | Explore Information Services"
date: "2021-09-28"
output:
    ioslides_presentation:
        smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(reticulate)
use_condaenv(condaenv = "r-reticulate")
print(py_config())
```

## Outline

- Overview
- R - summarytools
- Python - Pandas Profiling
- Python - Great Expectations
- Additional References
- Questions

# Overview

## Motivation

Data is important for actuarial work.

Irrelevant and poor quality data can lead to a tremendous amount of lost time, poor conclusions, and other bad outcomes.

We'll talk through ways to quickly and easily evaluate data quality before actuarial analyses begin.

The source, fictional data, and presentation are available at https://github.com/ActuarialHero/data_quality

We'll make active use of polling questions and the chat today, so be ready!

## My Relevant Background

In my role at a data vendor & credit reporting agency, I exchange data with numerous insurance carriers on a regular basis.

The data we receive varies drastically in quality, from truly immaculate to needing a replacement file.

My team and I use the tools we'll discuss to quickly review the data we receive and the data we return.

## ASOP 23 - Data Quality

ASOP 23 provides guidance to actuaries when

1. selecting data,
2. performing a review of data, 
3. using data, or 
4. relying on data supplied by others,

in performing actuarial services.

## ASOP 23 - Definitions

**Appropriate Data** - Data suitable for the intended purpose of an analysis and relevant to the system or process being analyzed.

**Data** - Numerical, census, or classification information, or information derived mathematically from such items, but not general or qualitative information. Assumptions are not data, but data are commonly used in the development of actuarial assumptions.

**Data Element** - An item of information, such as date of birth or risk classification.

**Review** - An examination of the obvious characteristics of data to determine if such data appear reasonable and consistent for purposes of the assignment. A review is not as detailed as an audit of data.

**Sufficient** - Containing enough data elements or records for the analysis.

## Poll Question 1

Is your company, principal, or client's data perfect?

1. Yes
2. No
3. Maybe

## In the chat...

What are some examples of data issues that you've discovered when you've reviewed data?

## Data Sources for this presentation

We'll work through the following example data sets today:

1. Reserving data from Glenn Meyer's excellent Monograph. Available from the [CAS website](https://www.casact.org/publications-research/research/research-resources/loss-reserving-data-pulled-naic-schedule-p).
2. A synthetic (randomly generated, somewhat representative) modeling data set.

## Poll Question 2

What tools do you normally use for working with data? [Multi-choice]

1. Excel
2. R
3. Python
4. SAS
5. Other - [Specify in chat]

# R summarytools Package

## Overview

From the website:

"summarytools is a an R package for data cleaning, exploring, and simple reporting. The package was developed with the following objectives in mind:

* Provide a coherent set of easy-to-use descriptive functions that are akin to those included in commercial statistical software suites such as SAS, SPSS, and Stata
* Offer flexibility in terms of output format & content
* Integrate well with commonly used software & tools for reporting (the RStudio IDE, Rmarkdown, and knitr) while also allowing for standalone, simple report generation from any R interface"

## Reserving Data

This section is based on the reserving data set constructed by Glenn Meyers and posted on the CAS website. The data is in CSV format with the following variables:

* **GRCODE** - NAIC company code (including insurer groups and single insurers)
* **GRNAME** - NAIC company name (including insurer groups and single insurers)
* **AccidentYear** - Accident year
* **DevelopmentYear** - Development year
* **DevelopmentLag** - Development lag (AY-1987 + DY-1987 - 1)
* **IncurLoss_** - Incurred losses and allocated expenses reported at year end
* **CumPaidLoss_** - Cumulative paid losses and allocated expenses at year end

## Reserving Data (cont.)

This section is based on the reserving data set constructed by Glenn Meyers and posted on the CAS website. The data is in CSV format with the following variables:

* **BulkLoss_** - Bulk and IBNR reserves on net losses and defense and cost containment expenses reported at year end
* **PostedReserve97_** - Posted reserves in year 1997 taken from the Underwriting and Investment Exhibit - Part 2A, including net losses unpaid and unpaid loss adjustment expenses
* **EarnedPremDIR_** - Premiums earned at incurral year - direct and assumed
* **EarnedPremCeded_** - Premiums earned at incurral year - ceded
* **EarnedPremNet_** - Premiums earned at incurral year - net
* **Single** 1 = single entity and 0 = group insurer

## Getting the data into R

Here are four commonly-used packages for importing data into R:

* **readr** - For importing rectangular data (like comma separated values, tab separated values, and fixed width format)
* **readxl** - For importing .xls and .xlsx files into R without the need for external dependencies (like xlsx or xlsReadWrite).
* **RODBC** - For importing data from databases using ODBC.
* **odbc** - A newer package for connecting to databases via ODBC.

R and Pandas in Python do a pretty good job of guessing field types, which makes loading data in pretty smooth. You can also manually specify field formats or edit them when they're in R/Python if needed.

## Getting the data into R

Today, we'll use the **read_csv** function in the **readr** package to read in the Private Passenger Auto reserving data since it's in CSV format. There are a few options for reading in CSV files, but read_csv does a pretty good job at guessing fields. That said, we can override read_csv's guesses if needed.

```{r, echo = TRUE}
library(readr)
pp_auto <- read_csv("../../data_in/meyers/ppauto_pos.csv")
```

## Profiling: Code

My team uses **dfSummary** from the **summarytools** package to examine data once it's in R. 

With a single line of code, dfSummary generates a nicely formatted HTML file with information about the data set and each of its fields. Snippets from this about are split across the next several slides.

```{r, echo = TRUE}
library(summarytools)
view(dfSummary(pp_auto), file = "../../data_out/pp_auto_profile.html")
```

## Profiling: Record Counts and Duplicates

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_01.png")
```

There 14,600 observations in the data set, and none are duplicates.

## Profiling: GRCODE

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_02.png")
```

- The variable was treated as numeric. This may or may not be appropriate since it's a code. Some, such as ZIP Codes, may need leading Zeros.
- Since it was numeric, it generated numeric summary statistics that we can ignore.
- There are 146 distinct values. The Monograph discuses 50 carriers for each line, so they are probably a subset of this data.

## Profiling: GRNAME

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_03.png")
```

- GRNAME is treated as a character variable
- The names look like the names of insurer groups
- The values are sorted from most to least common, with ties broken by alphabetical order. We see that the first 10 values appear 100 times each.
- No values are missing

## Profiling: AccidentYear

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_04.png")
```

- AccidentYear is treated as a numeric variable
- They run from 1988 - 1997
- Each AccidentYear appears 1460 times. Note that we don't normally expect the most recent accident year to have the same number of observations, so this means this triangle may have completed it's development.

## Profiling: DevelopmentYear

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_05.png")
```

- This distribution looks a little strange. I would have expected a symmetrical distribution. We can take a closer look with the dplyr package.

## Profiling: DevelopmentYear

```{r, echo = TRUE, message=FALSE}
library(dplyr)
pp_auto %>% count(DevelopmentYear) %>% head(n = 10)
```

## Profiling: DevelopmentYear

```{r, echo = TRUE, message=FALSE}
pp_auto %>% count(DevelopmentYear) %>% tail(n = 10)
```

## Profiling: DevelopmentLag

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_06.png")
```

- Development lag (or sometimes referred to as age) is uniform. This confirms our earlier intuition that the triangle may have completed development.

## Profiling: IncurLoss_B & CumPaidLoss_B

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_07.png")
```

- The distributions are skewed.
- Some values are negative in both fields. Let's take a look.

## Profiling: CumPaidLoss_B

```{r, echo = TRUE}
pp_auto %>%
    filter(IncurLoss_B < 0) %>%
    select(GRNAME, AccidentYear, IncurLoss_B) %>%
    arrange(IncurLoss_B) %>%
    head(n = 9)
```

## Profiling: CumPaidLoss_B

```{r, echo = TRUE}
pp_auto %>%
    arrange(desc(IncurLoss_B)) %>%
    select(GRNAME, AccidentYear, IncurLoss_B) %>%
    head(n = 10)
```

## Profiling: EarnedPremDIR_B & EarnedPremiumCeded_B

Let's skip a more detailed review of CumPaidLoss_B and BulkLoss_B and head straight for the premium variables:

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_08.png")
```

- These are also skewed.
- There are a few negatives. We'd probably want to peak to make sure they aren't unreasonable.

## Profiling: EarnedPremNet_B

And next is our net premium variable:

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_09.png")
```

- Similar observations to the other slides premium variables, but we should check to make sure Direct, Ceded, and Net all tie out.

## Profiling: Premium Balancing

```{r, echo = TRUE}
pp_auto %>% 
    mutate(PremBalAbs = abs(EarnedPremNet_B - (EarnedPremDIR_B - EarnedPremCeded_B))) %>%
    filter(PremBalAbs != 0) %>%
    count(GRNAME, PremBalAbs, sort = TRUE) %>% head(n = 10)
```

## Profiling: Single

There are more single companies than groups in this data set:

```{r, out.width = "800px"}
knitr::include_graphics("../../images/pp_auto_10.png")
```

We should also make sure none of the codes has both the single and group values:

```{r, echo = TRUE}
pp_auto %>% 
    distinct(GRCODE, Single) %>%
    count(GRCODE) %>%
    filter(n > 1)
```

## R summarytools - Summary

In just a couple of lines of code, we have a good feel for the data set. We were able to verify:

1. The number of records in the data set,
2. Each record is distinct,
3. The range of values in each field, and
4. The distribution of the values of numeric fields.

# Python pandas-profiling

## Overview

Similar to summarytools for R users, Python users can use Pandas profile. This example uses a fictional list of Kansas personal auto policies.

```{python, echo = TRUE, message = FALSE, results='hide'}
import pandas
from pandas_profiling import ProfileReport

pp_auto = pandas.read_csv("../../data_mid/auto_policy.csv")
profile = ProfileReport(pp_auto, title = "Auto Policy Profile")
profile.to_file("../../data_out/auto_policy_profile.html")

```

## Auto Driver/Policy Profile

Here's the same data set with driver information attached:

```{python, echo = TRUE, message = FALSE, results='hide'}
import pandas
from pandas_profiling import ProfileReport

pp_auto = pandas.read_csv("../../data_mid/auto_policy_driver.csv")

pp_auto = pp_auto.astype({"claim_count": float})

profile = ProfileReport(pp_auto, title = "Auto Policy Driver Profile")
profile.to_file("../../data_out/auto_policy_driver_profile.html")

```

## Pandas profiling - Summary

In just a couple of lines of code, we have an even better feel for the data set. We were able to verify:

1. The number of records in the data set,
2. The number of variables in the data set,
3. Frequency of missing values,
4. Frequency of duplicate values,
5. The number of each type of variable: Numerical, Boolean, and Categorical,
6. The range of each variable,
7. The distribution of each variable, including high cardinality, and 
8. Correlations between variables

# Python - Great Expectations

## Overview

Finally, Great Expectations is a python package for Validating, Documenting, and profiling data by setting expectations.

## What Are Expecations?

Expectations can be anything. Some ones that may be relevant to insurance data:

* The data set should include fields for Policy Number, Policy Year, Premiums, and Losses
* The amount of the claim should not be negative
* The policy limit should be between $1M and $10M
* Policy state should be in the following states: KS, MN
* Driver's age should not be null
* 95% of driver ages fall between 16 and 100
* The policy form should be HO-3.

## In the chat...

What are some expectations you would have on the fields in your data set?

## Setting Expectations

There are three options for setting expectations:

1. Manually, without interacting with a sample batch of data (default)
2. Interactively, with a sample batch of data
3. Automatically, using a profiler

We typically create them with a profiler and then edit them as necessary.

## Great Expectations In Modeling

You create expectations based on the data you built a model on, and then run data that is scored in production through the expectation set to verify that the model was built on appropriate data. Some examples that might fail expectations:

1. The model is being used in states that weren't considered during the initial model build.
2. The distribution of property values or liability limits varies drastically from the initial 
3. The cleanliness of the quotes being scored varies drastically from the cleanliness of written policies that were modeled. Drivers getting quotes tend to have more DUIs than drivers you insure.

## Great Expectations In Aggregating Data

When you need to aggregate data, you can build expectations on one set and run them on others to see how similar they are.

Example scenarios:

- You may need to combine data from multiple systems. You may discover that the state codes are different between systems.
- You are aggregating data from multiple insurance companies 
- You are creating industry benchmarks and have many different (non-insurance) companies submitting their loss data.

## Great Expectations - Advantages and Disadvantages

Advantages:

- Flexibility
- More easily compares against other data sets
- Handles data sets larger than memory

Disadvantages:

- Installing the package is more difficult
- Setting up data expectations takes time

# Review

## Review

We've discussed three tools today:

- R - summarytools
- Python - Pandas Profiling
- Python - Great Expectations

Any of them would be a great start, and take minimal work, provided your company uses Python and/or R.

If you company has a preference of R or Python, lean toward those tools.

# Additional references

## Additional references

* CAS
    + [CAS Monograph No. 1: Stochastic Loss Reserving Using Bayesian MCMC Models](https://www.casact.org/monograph/cas-monograph-no-1)
    + [CAS Monograph No. 8: Stochastic Loss Reserving Using Bayesian MCMC Models (2nd Edition)](https://www.casact.org/monograph/cas-monograph-no-8)
    + [CAS Monograph No. 9: Data Quality Management in the P&C Insurance Sector](https://www.casact.org/monograph/cas-monograph-no-9)
* R
    + [RMarkdown Guide](https://rmarkdown.rstudio.com/articles.html)
    + [R for Data Science](https://r4ds.had.co.nz/) by Hadley Wickham
    + [Tidyverse Website](https://www.tidyverse.org/)
    + [summarytools GitHub Page](https://github.com/dcomtois/summarytools)
* Python
    + [Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling)
    + [Great Expectations](https://greatexpectations.io/)

# Questions

## Questions

- Overview
- R - summarytools
- Python - Pandas Profiling
- Python - Great Expectations
- Additional References
- Questions